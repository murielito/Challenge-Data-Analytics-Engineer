# -*- coding: utf-8 -*-
"""Prueba_Meli.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ra42_oCsbmShizyDSSS1J097n8OGEraA

# Análisis exploratorio
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('df_challenge_meli.csv')

#Aseguramos reproducibilidad global en el código, no solo en los modelos
np.random.seed(42)

df.head()

#Verificar que haya cargado la cantidad de filas y columnas que vimos en el csv, en este caso 185250 filas y 14 columnas
df.shape

#Queremos saber que tipos de datos vienen en la base para su tratamiento
df.info()

#Se quiere mirar el comportamiento general de las variables númericas para saber como encarar el modelo de clusterización y mitigar efectos de outliers
df.describe()

#Validar el distinct de valores en cada columna
for col in df.columns:
    print(f"{col}: {df[col].nunique()} valores únicos")

df.category_name.value_counts()

df.seller_reputation.value_counts()

df.isnull().sum()
# la cantidad de regular_price null puede dejar muchos productos sin descuento

# La diagonal con los histogramas alerta de los outliers con valores altisimos
# En la siguiente línea se testean dos histogramas con muestras para poder ver una distribución más clara
num_cols = ['price','regular_price','stock']

muestra = df.sample(500)
sns.pairplot(muestra[num_cols], diag_kind="hist")

plt.hist(df.price, bins=50, range=(0, 10000))
plt.title('Histograma de Precios')
plt.show()

plt.hist(df.stock, bins=50, range=(0, 100))
plt.title('Histograma de Stock')
plt.show()

#Escaneo de precios, dado que en el histograma inicial todo se concentraba en valores cercanos a 2000. Presencia de valores muy extremos vs la media
df.price.sort_values(ascending=False).iloc[0:1000]

df.stock.sort_values(ascending=False).iloc[0:1000]
(df['stock'] == 0).sum()

df.groupby('seller_nickname')['stock'].value_counts().nlargest(20).plot(kind='bar')

cat_cols = ['seller_reputation','logistic_type','condition']
for col in cat_cols:
    plt.figure(figsize=(8, 4))
    sns.countplot(data=df, x=col, order=df[col].value_counts().index)
    plt.xticks(rotation=45)
    plt.title(f"Conteo por {col}")
    plt.show()

##top_10 = df['category_name'].value_counts().nlargest(10)

df['category_name'].value_counts().nlargest(20).plot(kind='bar', color='skyblue')
plt.title("Top 20 categorías")
plt.xlabel("Categoría")
plt.ylabel("Cantidad de ítems")

media = df['category_name'].value_counts().nlargest(20).mean()
plt.axhline(media, color='red', linestyle='--', label=f'Media: {media:.0f}')

# Mostrar leyenda y gráfico
plt.legend()
plt.show()

"""Consideraciones del análisis exploratorio de datos:
Hay 6033 filas donde no hay stock de los productos, sin embargo no se borraron dado que el análisis es por vendedor y no por producto.
Adicionalmente, el precio tiene un rango demasiado amplio por precios muy altos, esto puede influir en el resultado posterior del modelo, sin embargo, no se eliminaron para ver las diferentes distribuciones

# Feature Engineering
"""

df.head()

df.drop(['tim_day','titulo','url','categoria','category_name'],axis=1, inplace=True)

df_1 = df.copy()
df_1.head()

df_1['seller_reputation'] = df_1['seller_reputation'].fillna('otro')
df_1.seller_reputation.value_counts()

#Creamos reputaciones numéricas para cada categoría
reputation_map = {
    'green_platinum': 10,
    'green_gold': 9,
    'green_silver': 8,
    'green': 7,
    'light_green': 6,
    'red': 5,
    'orange': 4,
    'yellow': 3,
    'newbie': 2,
    'otro': 1
}

df_1['reputation_score'] = df_1['seller_reputation'].map(reputation_map)
df_1.head()

#Creamos una columna que puede ser entendida como productos con descuento cuando el precio es menor al precio regular del catalogo
#Asumimos que cuando el precio regular no viene (Nan) se toma el precio actual
#Se convierten las categorias en números para el modelo, aunque no se vayan a usar todas es mejor categorizarlas numéricamente
df_1['has_discount'] = np.where(df_1['price'] < df_1['regular_price'], 1, 0).astype(int)
df_1['is_refurbished'] = df_1['is_refurbished'].apply(lambda x: 1 if x == True else 0)
df_1['condition'] = df_1['condition'].apply(lambda x: 1 if x == 'new' else (2 if x == 'used' else 3))
df_1['logistic_type'] = df_1['logistic_type'].apply(lambda x: 1 if x == 'FBM' else (2 if x == 'DS' else (3 if x == 'XD' else 4)))
df_1.head()

df_1.head()
df_1.logistic_type.value_counts()

#Asumimos que donde no venía regular price se toma el precio de catalogo, en este caso concuerda la lógica del descuento
#Creamos el logaritmo de price y stock por su distribución asimétrica y outliers en el extremo superior
#Usamos log1p en lugar de log por los valores en 0
df_1['regular_price'] = df_1['regular_price'].fillna(df_1['price'])
df_1['log_price'] = np.log1p(df_1['price'])
df_1['log_stock'] = np.log1p(df_1['stock'])
df_1.head()

plt.hist(df_1.log_price, bins=50)
plt.title('Histograma de Precios')
plt.show()

plt.hist(df_1.log_stock, bins=20)
plt.title('Histograma de Stock')
plt.show()

"""#Modelo de Clustering"""

df_sellers = df_1.groupby('seller_nickname').agg(
    avg_prod_price = ('log_price', 'mean'),
    max_prod_price = ('log_price', 'max'),
    ##std_prod_price = ('log_price', 'std'),
    avg_stock = ('log_stock', 'median'),
    unique_cat = ('category_id', 'nunique'),
    reputation_score = ('reputation_score', 'mean'),
    ##condition_new_ratio = ('condition', lambda x: (x == '1').mean()),
    has_discount_ratio = ('has_discount', 'mean')
).reset_index()

df_sellers = df_sellers.fillna(0)

df_sellers.head()

df_sellers.has_discount_ratio.value_counts()

"""Escala y normalización"""

from sklearn.preprocessing import StandardScaler

variables_a_escalar = df_sellers.drop('seller_nickname', axis=1).columns
scaler = StandardScaler()
var_escaladas = scaler.fit_transform(df_sellers[variables_a_escalar])
df_escaladas = pd.DataFrame(var_escaladas, columns=variables_a_escalar)
df_escaladas.head()

#Esto es un control para ver que la escala normalización y escala se haya aplicado correctamente
print(f'Media del df con var escaladas:', df_escaladas.mean().round(3))
print(f'Desviación estándar del df con var escaladas:', df_escaladas.std().round(3))

from sklearn.cluster import KMeans

modelo = KMeans(n_clusters=3, random_state=42)
clusters = modelo.fit_predict(df_escaladas)
df_sellers['cluster_modelo'] = clusters

print(df_sellers['cluster_modelo'].value_counts())

df_sellers['cluster_modelo'] = df_sellers['cluster_modelo'].astype(int)
df_sellers.dtypes

df_sellers_1 = df_sellers.copy()

df_sellers_1.drop('seller_nickname', axis=1, inplace=True)

resumen_cluster = df_sellers_1.groupby('cluster_modelo').mean().round(2)
print(resumen_cluster)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 8))
sns.scatterplot(
    x='avg_prod_price',
    y='avg_stock',
    hue='cluster_modelo',
    data=df_sellers,
    palette='viridis',
    s=100
)
plt.title('Visualización de Clústeres (Escala Logarítmica)')
plt.xlabel('Precio promedio del producto')
plt.ylabel('Stock promedio')

# Aplica escala logarítmica a los ejes para manejar la alta varianza
plt.xscale('log')
plt.yscale('log')

plt.show()

"""Evaluación de clústers

"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertia = []

for k in range(2,11):
    cluster_optimo = KMeans(n_clusters=k, random_state=42, n_init = 10)
    cluster_optimo.fit(df_escaladas)
    inertia.append(cluster_optimo.inertia_)

plt.figure(figsize=(8, 6))
plt.plot(range(2,11), inertia, marker='o')
plt.xlabel('Número de Clusters (K)')
plt.ylabel('Inertia')
plt.title('Método del codo')
plt.show()

"""Según los resultados de la gráfica, se sugiere usar 4 clústeres porque la inclinación de ahí en adelante es menos pronunciada y haría cada grupo menos compacto

El argumento n_init se estableció en 10 iteraciones, encontrando el resultado del análisis de la gráfica más pertinente que con un número menor de iteraciones.
"""

from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans

for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(df_escaladas)
    score = silhouette_score(df_escaladas, labels)
    print(f"k={k}: Silhouette Score={score:.3f}")

"""Se midió la calidad del clúster con el Silhouette Score calculando que tan similares son los puntos entre su cluster y que tan difenrente de los otros clusters. Si bien todos gravitan alrededor de 0.26 no siempre un score alto garantiza el número óptimo de k's.

En este caso consideré apropiado junto con el método el codo asignar 4 grupos distintos (score = 0.25), dado que de cara a generar estrategias comerciales tener más grupos podría desencadenar en una poca diferenciación y ambiguedad entre grupos.

Modelo con el número óptimo de k
"""

# Usamos el modelo con el número óptimo de clústeres
# Su usa el mismo número de inicializaciones, con una semilla de 42 para asegurar la reproducibilidad del resultado
from sklearn.cluster import KMeans

modelo = KMeans(n_clusters=4, random_state=42)
clusters = modelo.fit_predict(df_escaladas)
df_sellers['cluster_modelo'] = clusters

print(df_sellers['cluster_modelo'].value_counts())

df_sellers['cluster_modelo'] = df_sellers['cluster_modelo'].astype(int)
df_sellers.dtypes

"""
Clusters usados para las estrategias y la presentación:
0: 17808
3: 11856
1: 10088
2: 6834
 """

df_sellers_2 = df_sellers.copy()

df_sellers_2.drop('seller_nickname', axis=1, inplace=True)

resumen_cluster = df_sellers_2.groupby('cluster_modelo').mean().round(2)
print(resumen_cluster)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 8))
sns.scatterplot(
    x='avg_prod_price',
    y='avg_stock',
    hue='cluster_modelo',
    data=df_sellers,
    palette='viridis',
    s=100
)

plt.title('Visualización de Clústeres')
plt.xlabel('Precio promedio del producto')
plt.ylabel('Stock promedio')

plt.xscale('log')
plt.yscale('log')

plt.show()

"""

# Extensión Gen AI

"""

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')

df_sellers['text_for_embedding'] = df_sellers.apply(
    lambda row: (
        f"El seller tiene un precio promedio de {row['avg_prod_price']} "
        f"(lo que es {'alto' if row['avg_prod_price'] > 6 else 'bajo'}), "
        f"stock promedio de {row['avg_stock']} unidades, "
        f"y aplica descuentos en el {row['has_discount_ratio']*100}% de sus productos. "
        f"Su reputación es {row['reputation_score']}/10 "
        f"y vende en {row['unique_cat']} categorías distintas."
    ),
    axis=1
)

df_sellers['text_for_embedding'][0]

df_sellers.head()

# Vectorizamos el texto de cada vendedor, dividido en lotes para que no se quede pegado el pc
# Se puede demorar ~30 min en correr esta celda
embeddings = model.encode(df_sellers['text_for_embedding'].tolist(), batch_size=32, show_progress_bar=True)

# Una vez creados los vectores númericos de las descripciones de los sellers, se corre la regresión logística para aprender a predecir a que cluster pertenecen los perfiles semánticos

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X = embeddings
y = df_sellers['cluster_modelo']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

reg_logistica = LogisticRegression(random_state=42)
reg_logistica.fit(X_train, y_train)
y_pred = reg_logistica.predict(X_test)

print(reg_logistica.score(X_test, y_test)*100)

# El accuracy nos dice que tan bien predice el modelo

prueba_vendedor = (
    "El seller tiene un precio promedio de 8.9 (lo que es alto), "
    "vende en 5 categorías distintas, "
    "su reputación es 6/10, "
    "tiene un stock promedio de 1.3 "
    "y aplica descuentos en el 25% de sus productos."
)

nuevo_vector = model.encode([prueba_vendedor])
prediccion_nuevo = reg_logistica.predict(nuevo_vector)
print("Este nuevo vendedor pertenece al clúster:", prediccion_nuevo[0])

##Ofrecer campañas de descuentos por volumen (si es un cluster de alto stock).
##Priorizar en búsquedas de productos con alta rotación.